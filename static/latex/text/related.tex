
\vspace{-1mm}
\section{Related Work}
\vspace{-2mm}
% In this section, we review the close-related topics, \emph{i.e.}, Large Language Models, Retrieval Augmented Generation and Offline RL.

% \vspace{-3mm}

% \subsection{Large Language Models for Decision-Makings}
% LLM用来输出策略的几种方式：（1）直接出策略（Language-based Decision-Makings：LLM-agent），下面的全是；（2）Language-model-assistant Decision-Makings：用LLM出目标，策略，reward，guide策略探索，加速策略学习，like：RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds；（3）Language-guided Decision-Makings：让策略condition on language，让策略根据人类的instruction完成不同的行为，是一种goal condition的policy。
% 我觉得区别直接讲，我们的方法是独立于上面的所有范式的新的范式，Language-derived Decision-Makings。我们直接基于语言learn一个策略出来。不用纠结于那些细节。

% \textbf{Large Language Models (LLMs).} 
LLMs have demonstrated remarkable potential in achieving human-level ability in various tasks, sparking a surge in studies investigating LLM-based autonomous agents. Based on the different roles of LLM in the agents, there are two types of work.  
% LLM can be used as a module to improve existing or be used as an actionable agent directly~\citep{voyager2023guanzhi}. Early works prefer open-loop plans and hard-code search algorithms to improve the long-term planning ability of LLM. Chain of Thought~\citep{cot2022jason} proposes inputting reasoning steps for solving complex problems into the prompt. Closed-loop planning leverages environmental feedback, thereby facilitating more adaptive decision-making. 
%  ReAct~\citep{react@2022shunyu} and Inner Monologue~\citep{inner@2022wenlong} allow
% LLM agents take single-step actions according to the environmental feedback. Reflexion~\citep{reflexion@2023noah}, further allows ReAct agent to revise itself from past trials and errors.
Firstly, LLM can be used 
% as a module to accelerate and improve existing planning/RL methods or 
as an actionable agent directly~\citep{voyager2023guanzhi}. Early works~\citep{cot2022jason, llm-mcts2023zirui} prefer open-loop plans and hard-code search algorithms to improve the long-term planning ability of LLM. Closed-loop planning~\citep{react@2022shunyu, inner@2022wenlong, reflexion@2023noah} leverages environmental feedback, thereby facilitating more adaptive decision-making. 
%  ReAct~\citep{react@2022shunyu} and Inner Monologue~\citep{inner@2022wenlong} allow
% LLM agents take single-step actions according to the environmental feedback. Reflexion~\citep{reflexion@2023noah}, further allows ReAct agent to revise itself from past trials and errors.
Secondly, LLM can be used to assist or accelerate the learning of agents. The form of assistance can be quite diverse. It can generate high-level plans~\citep{liu2023llm+, llmplanner@2023song}, rewards~\citep{drlc@2024cao, kwon2023reward, wu2024read}, transitions~\citep{zala2024envgen, ren2024bases}, or as an adapter to convert human instructions into structured inputs~\citep{hu2023instructrl}. In our work, LLMs play a different role compared to all the above-mentioned types. LLMs are to understand the knowledge in the books, rehearsing the decision-making process to derive an imaginary dataset based on the knowledge.  The policy to execute is a neural network distilled from the imaginary data without interactions with the real environments. 
