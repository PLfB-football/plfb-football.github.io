<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting">
  <meta name="keywords" content="Policy Learning from Tutorial Books, PlfB, URI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/football-svgrepo-com.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://xionghuichen.github.io/">Xiong-Hui Chen</a><sup><b>*</b>,1</sup>,</span>
            <span class="author-block">
              <a href="https://ziyan-wang98.github.io/">Ziyan Wang</a><sup><b>*</b>,2</sup>,</span>
            <span class="author-block">
              <a href="https://yalidu.github.io/">Yali Du</a><sup>2</sup>,
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=v2eJuUkAAAAJ&hl=en">Shengyi Jiang</a><sup>4</sup>,
            </span>
            </span>
            <span class="author-block">
              <a href="https://mengf1.github.io/">Meng Fang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.uk/citations?user=PG2lDSwAAAAJ&hl=en">Yang Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=wIE1tY4AAAAJ&hl=en">Jun Wang</a><sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanjing University,</span>
            <span class="author-block"><sup>2</sup>King's College London,</span>
            <span class="author-block"><sup>3</sup>University of Liverpool,</span>
            <span class="author-block"><sup>4</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>5</sup>University College Lonodn,</span>
          </div>
          
          <!-- * is for same contribution -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">* Equal contribution</span>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/21cf8411ed825614e00006a1d9aab7e4-Paper-Conference.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://neurips.cc/virtual/2024/poster/96082"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>NeurIPS</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://github.com/ziyan-wang98/URI_video_NeurIPS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/xionghuichen/plfb-uri"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.2em;">
            When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMs’ systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network. We build two benchmarks for PLfB~based on Tic-Tac-Toe and Football games. In experiment, URI's policy achieves at least 44% net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI's policy beat the built-in AIs with a 37% while using GPT-based agent can only achieve a 6\% winning rate.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- TODO: add the talk video -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Policy Learning from Books</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
          <img src="./static/images/PER-problem.png" alt="Comparison of Policy Learning Methods">
          <!-- <figcaption class="has-text-centered" style="font-size: 1.1em;">Figure: Comparison of policy learning from books versus learning from data</figcaption> -->
        </figure>
        <div class="content has-text-justified">
          <p style="font-size: 1.2em;">
            In this study, we introduce a novel topic built upon the shoulders of LLMs' systems: Policy Learning from Books (<strong>PLfB</strong>). <strong>PLfB</strong> aims to derive a policy network directly from natural language texts, bypassing the need for numerous real-world interaction data, as shown in this figure. This can be viewed as a further step towards enabling more resources for policy learning and also a more generalized form of offline RL, which uses textbooks to learn a policy offline.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Method</h2>
    <div class="content has-text-justified">
      
      <figure class="image">
        <img src="./static/images/framework_new.drawio (1).png" alt="URI Framework Detailed Overview">
        <!-- <figcaption class="has-text-centered" style="font-size: 1.1em;">Figure: Detailed overview of the URI (Understanding, Rehearsing, Introspecting) framework</figcaption> -->
      </figure>
      <p style="font-size: 1.2em;">
        To realize <strong>PLfB</strong>, inspired by human learning processes, we propose a three-stage learning methodology: Understanding, Rehearsal, and Introspection (<strong>URI</strong>), which is shown in Figure above.  
        For understanding, it first extracts knowledge from books to form a knowledge database; then it rehearses imaginary decision-making trajectories with the help of the knowledge retrieved from the database; finally, it introspects on the imaginary dataset to distill a policy network for decision-making.
      </p>

      <p style="font-size: 1.2em;">Our proposed URI framework consists of three main stages:</p>
      <ol style="font-size: 1.2em;">
        <li><strong>Understanding:</strong> The knowledge extractor and aggregator modules process paragraphs from books to form a structured knowledge database organized as pseudo-code.</li>
        <li><strong>Rehearsing:</strong> Using the knowledge database, the simulator generates and iterates through imagined states, actions, and rewards to create an extensive imaginary dataset.</li>
        <li><strong>Introspecting:</strong> The introspection module refines the policy network by evaluating and correcting errors in the generated states, actions, and rewards to ensure accurate and effective policy implementation.</li>
      </ol>

      We give the first implementation for URI framework: 

      <figure class="image">
        <img src="./static/images/phrase-1.png" alt="URI">
        <!-- <figcaption class="has-text-centered" style="font-size: 1.1em;">Figure: Detailed overview of the URI (Understanding, Rehearsing, Introspecting) framework</figcaption> -->
      </figure>
      
      <!-- <div class="content">
        <h3 class="title is-4">Detailed Implementation</h3> -->
        
        <div class="tabs">
          <ul>
            <li class="is-active" data-tab="tab1"><a>Book Content Understanding</a></li>
            <li data-tab="tab2"><a>Knowledge-based Rehearsing</a></li>
            <li data-tab="tab3"><a>Introspecting based on the Imaginary Dataset</a></li>
          </ul>
        </div>
        
        <div class="tab-content">
          <div class="tab-pane is-active" id="tab1">
            <h4>Book Content Understanding</h4>
            <p>
              The understanding module extracts knowledge K from books B using a knowledge extractor and aggregator. Knowledge is represented as pseudo-code, which is interpretable, compact, and expressive. The process involves:
            </p>
            <ul>
              <li>Using an LLM-injected model to extract knowledge from paragraphs</li>
              <li>Aggregating knowledge using another LLM-injected model</li>
              <li>Iteratively refining and organizing knowledge into dynamics, reward, and policy-related databases</li>
            </ul>
          </div>
          
          <div class="tab-pane" id="tab2">
            <h4>Knowledge-based Rehearsing of Decision-Making</h4>
            <p>
              The rehearsing stage implements a closed-loop generation process involving LLM-injected dynamics, reward, and policy functions. Key components include:
            </p>
            <ul>
              <li>State-based Knowledge Scope Retrieval to address modality gaps</li>
              <li>Post-Retrieval Knowledge Instantiation for tailoring knowledge to specific states</li>
              <li>Generation of imaginary dataset using LLMs for actions, rewards, and next states</li>
            </ul>
          </div>
          
          <div class="tab-pane" id="tab3">
            <h4>Introspecting based on the Imaginary Dataset</h4>
            <p>
              The introspection stage refines the policy using the imaginary dataset. It involves:
            </p>
            <ul>
              <li>Adopting Conservative Q-learning as the base offline RL algorithm</li>
              <li>Adding uncertainty penalties for reward and transition estimations</li>
              <li>Implementing Conservative Imaginary Q-Learning (CIQL) to address inaccuracies in LLM-generated data</li>
            </ul>
          </div>
        </div>
      </div>






<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Key Results</h2>
    <div class="content has-text-justified">
      <ul>
        <li>URI outperforms baseline methods in both Google Football and Tic-Tac-Toe games:</li>
        <ul>
          <li>In Google Football 11v11 full-game scenarios, URI excels across all difficulty levels.</li>
          <li>In the Hard task of Google Football, URI achieves a higher win rate than the Rule-based Policy.</li>
          <li>In Tic-Tac-Toe, URI demonstrates superior performance against various opponents.</li>
        </ul>
        <li>URI significantly outperforms LLM-based methods in inference time, taking only 0.009 seconds on average to choose an action in Google Football.</li>
        <li>Visualization of the imaginary dataset shows that generated data follows a similar distribution to real data, and uncertainty estimation identifies out-of-distribution clusters.</li>
      </ul>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Key Results</h2>
    <div class="content has-text-justified">
      <h3 class="title is-4">1. Performance in Multiple Environments</h3>



      <h4 class="title is-5">🎲 Tic-Tac-Toe Game </h4>
      <ul>
        <li>URI shows superior performance against various opponents.</li>
        <li>URI's performance is closer to the optimal Minimax strategy compared to other methods.</li>
      </ul>
      
      <table class="table is-striped is-narrow is-hoverable is-fullwidth">
        <caption>Performance Comparison in Tic-Tac-Toe (URI as Player X)</caption>
        <thead>
          <tr>
            <th>Opponent (O)</th>
            <th>Win</th>
            <th>Draw</th>
            <th>Loss</th>
            <th>Net Win Rate</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>LLM-as-agent</td>
            <td>80%</td>
            <td>6%</td>
            <td>14%</td>
            <td>+66%</td>
          </tr>
          <tr>
            <td>LLM-RAG</td>
            <td>62%</td>
            <td>10%</td>
            <td>18%</td>
            <td>+44%</td>
          </tr>
          <tr>
            <td>Random Policy</td>
            <td>70%</td>
            <td>12%</td>
            <td>18%</td>
            <td>+52%</td>
          </tr>
          <tr>
            <td>Minimax-noise</td>
            <td>36%</td>
            <td>54%</td>
            <td>10%</td>
            <td>+26%</td>
          </tr>
        </tbody>
      </table>

      
      <h4 class="title is-5">⚽ Google Football (11v11)</h4>
      <ul>
        <li>URI outperforms baseline methods across all difficulty levels (Easy, Medium, Hard).</li>
        <li>In the Hard task, URI achieves a higher win rate than the Rule-based Policy.</li>
        <li>URI demonstrates consistent performance with an average Goal Difference per Match (GDM) of 0.38 ± 0.05 across all levels.</li>
      </ul>
      
      <table class="table is-striped is-narrow is-hoverable is-fullwidth">
        <caption>Performance Comparison Against Built-in AI Levels in GRF 11 vs 11 settings</caption>
        <thead>
          <tr>
            <th>Level</th>
            <th>Metric</th>
            <th>LLM-as-agent</th>
            <th>LLM-RAG</th>
            <th>Random Policy</th>
            <th>URI (Ours)</th>
            <th>Rule-based-AI</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="4">Easy</td>
            <td>Win</td>
            <td>20%</td>
            <td>30%</td>
            <td>2%</td>
            <td><strong>37% ± 4%</strong></td>
            <td>70%</td>
          </tr>
          <tr>
            <td>Draw</td>
            <td>60%</td>
            <td>60%</td>
            <td>55%</td>
            <td>57% ± 4%</td>
            <td>30%</td>
          </tr>
          <tr>
            <td>Lose</td>
            <td>20%</td>
            <td>10%</td>
            <td>43%</td>
            <td>6% ± 4%</td>
            <td>0%</td>
          </tr>
          <tr>
            <td>GDM</td>
            <td>0.0</td>
            <td>0.2</td>
            <td>-0.58</td>
            <td><strong>0.40 ± 0.14</strong></td>
            <td>0.7</td>
          </tr>
          <tr>
            <td rowspan="4">Medium</td>
            <td>Win</td>
            <td>0%</td>
            <td>20%</td>
            <td>2%</td>
            <td><strong>42% ± 12%</strong></td>
            <td>70%</td>
          </tr>
          <tr>
            <td>Draw</td>
            <td>60%</td>
            <td>60%</td>
            <td>43%</td>
            <td>50% ± 8%</td>
            <td>30%</td>
          </tr>
          <tr>
            <td>Lose</td>
            <td>40%</td>
            <td>20%</td>
            <td>55%</td>
            <td>8% ± 4%</td>
            <td>0%</td>
          </tr>
          <tr>
            <td>GDM</td>
            <td>-0.4</td>
            <td>0.0</td>
            <td>-0.76</td>
            <td><strong>0.43 ± 0.24</strong></td>
            <td>0.7</td>
          </tr>
          <tr>
            <td rowspan="4">Hard</td>
            <td>Win</td>
            <td>0%</td>
            <td>0%</td>
            <td>3%</td>
            <td><strong>32% ± 14%</strong></td>
            <td>30%</td>
          </tr>
          <tr>
            <td>Draw</td>
            <td>50%</td>
            <td>40%</td>
            <td>43%</td>
            <td>58% ± 6%</td>
            <td>70%</td>
          </tr>
          <tr>
            <td>Lose</td>
            <td>50%</td>
            <td>60%</td>
            <td>53%</td>
            <td>10% ± 7%</td>
            <td>0%</td>
          </tr>
          <tr>
            <td>GDM</td>
            <td>-0.5</td>
            <td>-0.6</td>
            <td>-0.73</td>
            <td><strong>0.32 ± 0.14</strong></td>
            <td>0.3</td>
          </tr>
          <tr class="is-selected">
            <td rowspan="2">Average</td>
            <td>Win</td>
            <td>6.7% ± 9.4%</td>
            <td>16.7% ± 12.5%</td>
            <td>2.3% ± 0.5%</td>
            <td><strong>40.3% ± 6.2%</strong></td>
            <td>56%</td>
          </tr>
          <tr class="is-selected">
            <td>GDM</td>
            <td>-0.30 ± 0.22</td>
            <td>-0.13 ± 0.34</td>
            <td>-0.69 ± 0.08</td>
            <td><strong>0.38 ± 0.05</strong></td>
            <td>0.56</td>
          </tr>
        </tbody>
      </table>
      


      <h3 class="title is-4">2. Efficiency</h3>
      <ul>
        <li>URI significantly outperforms LLM-based methods in inference time, taking only 0.009 seconds on average to choose an action in Google Football.</li>
      </ul>

      <h3 class="title is-4">3. Data Quality</h3>
      <ul>
        <li>Visualization of the imaginary dataset shows that generated data follows a similar distribution to real data.</li>
        <li>Uncertainty estimation effectively identifies out-of-distribution clusters, enhancing the robustness of the learned policy.</li>
      </ul>
      <div class="columns is-centered">
        <div class="column">
          <figure class="image">
            <img src="static/images/exp/tsne-1.png" alt="TSNE Visualization 1">
            <figcaption>Figure 1: t-SNE visualization of real and generated data distribution in football game</figcaption>
          </figure>
        </div>
        <div class="column">
          <figure class="image">
            <img src="static/images/exp/ttt-tsne.png" alt="TSNE Visualization 2">
            <figcaption>Figure 2: t-SNE visualization of real and generated data distribution in Tic-Tac-Toe game</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="columns is-vcentered">
        <div class="column is-half">
          <div class="content">
            <h2 class="title is-3">LLM-RAG Agent (Baseline)</h2>
            <p>
              This video demonstrates the performance of our LLM-RAG Agent baseline in a challenging 11 vs 11 Medium level football scenario. 
              The agent utilizes Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to make decisions in real-time gameplay.
            </p>
          </div>
          <div style="width: 100%; height: 0; padding-bottom: 56.25%; position: relative;">
            <video id="llm-rag-video" autoplay controls muted loop playsinline style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
              <source src="./static/videos/LLM-RAG/11_vs_11_Med/llm_rag_11v11_level_Med_3D.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column is-half">
          <div class="content">
            <h2 class="title is-3">URI Policy (Ours)</h2>
            <p>
              The URI Agent demonstrates superior performance compared to the LLM-RAG baseline, showcasing enhanced decision-making capabilities and a significantly higher goal-scoring rate. This improvement is particularly evident in the agent's ability to create and capitalize on scoring opportunities more effectively.
            </p>
          </div>
          <div style="width: 100%; height: 0; padding-bottom: 56.25%; position: relative;">
            <video id="uri-agent-video" autoplay controls muted loop playsinline style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" playbackRate="5">
              <source src="./static/videos/URI/11_vs_11_Med/URI_11v11_level_M_2-0_3D_1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <!--/ Visual Effects. -->

    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>

        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>

        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>

          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>

            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>

          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>

        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2024understanding,
      title={Understanding, Rehearsing, and Introspecting: Learn a Policy from Textual Tutorial Books in Football Games},
      author={Chen, Xiong-Hui and Wang, Ziyan and Du, Yali and Fang, Meng and Jiang, Shengyi and Yu, Yang and Wang, Jun},
      booktitle = {Advances in Neural Information Processing Systems},
      year={2024}
    }</code></pre>
  </div>
</section>


<style>
  .thin-footer {
    padding: 10px 0; /* Adjust this value to your liking */
  }
</style>

<footer class="footer thin-footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener('DOMContentLoaded', function () {
    var tabs = document.querySelectorAll('.tabs li');
    var tabPanes = document.querySelectorAll('.tab-pane');

    tabs.forEach(function (tab) {
      tab.addEventListener('click', function () {
        var target = this.dataset.tab;

        // 移除所有选项卡的激活状态
        tabs.forEach(function (item) {
          item.classList.remove('is-active');
        });

        
        this.classList.add('is-active');

        tabPanes.forEach(function (pane) {
          pane.classList.remove('is-active');
        });

        document.getElementById(target).classList.add('is-active');
      });
    });
  });
</script>

</body>
</html>
